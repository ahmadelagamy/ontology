{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Paper Implentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology Table:\n",
      "[{'text': '', 'type': 'Potential Entity', 'relation': '', 'attributes': [], 'superclass': None}, {'text': '', 'type': 'Potential Entity', 'relation': '', 'attributes': [], 'superclass': None}]\n",
      "\n",
      "NER Table:\n",
      "[{'text': '', 'type': ''}, {'text': '', 'type': ''}, {'text': '', 'type': ''}, {'text': '', 'type': ''}, {'text': '', 'type': ''}, {'text': '', 'type': ''}, {'text': '', 'type': ''}]\n",
      "\n",
      "Dependency Parsing Table:\n",
      "[{'word': 'The', 'pos': 'DET', 'dependency': 'det', 'head': 'queen'}, {'word': 'queen', 'pos': 'NOUN', 'dependency': 'nsubj', 'head': 'laying'}, {'word': 'is', 'pos': 'AUX', 'dependency': 'aux', 'head': 'laying'}, {'word': 'not', 'pos': 'PART', 'dependency': 'neg', 'head': 'laying'}, {'word': 'laying', 'pos': 'VERB', 'dependency': 'ROOT', 'head': 'laying'}, {'word': 'any', 'pos': 'DET', 'dependency': 'det', 'head': 'eggs'}, {'word': 'eggs', 'pos': 'NOUN', 'dependency': 'dobj', 'head': 'laying'}]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "\n",
    "# Define thresholds for superclass assignment (adjust as needed)\n",
    "higher_threshold = 0.3  # Threshold for first-level classes\n",
    "lower_threshold = 0.5  # Threshold for non-first-level classes\n",
    "\n",
    "\n",
    "def tag_noun_with_superclass(ontology, noun, higher_threshold= 0.3 , lower_threshold= 0.5):\n",
    "  \"\"\"\n",
    "  Assigns a superclass to a noun based on an OWL ontology and thresholds.\n",
    "\n",
    "  Args:\n",
    "      ontology (Ontology): The loaded OWL ontology.\n",
    "      noun (str): The noun to be tagged.\n",
    "      higher_threshold (float): Threshold for first-level classes.\n",
    "      lower_threshold (float): Threshold for non-first-level classes.\n",
    "\n",
    "  Returns:\n",
    "      str: The superclass tag for the noun (or None if not tagged).\n",
    "  \"\"\"\n",
    "\n",
    "  # Check if noun corresponds to an ontology class (consider label or IRI)\n",
    "  for cls in ontology.classes():\n",
    "    if noun in cls.label or noun == cls.iri:\n",
    "      return str(cls.is_a[0])  # Return first superclass\n",
    "\n",
    "  # Otherwise, no direct class match, so skip\n",
    "\n",
    "  return None\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def analyze_sentence(sentence, ontology_file_path):\n",
    "  \"\"\"\n",
    "  Analyzes a sentence and outputs ontology, NER, and dependency parsing data with\n",
    "  improvements for relation/attribute identification, lexical resources, coreference\n",
    "  resolution, disambiguation, and superclass tagging.\n",
    "\n",
    "  Args:\n",
    "      sentence (str): The sentence to be analyzed.\n",
    "      ontology_file_path (str): The path to the OWL ontology file.\n",
    "\n",
    "  Returns:\n",
    "      dict: A dictionary containing the ontology, NER, and dependency parsing results.\n",
    "  \"\"\"\n",
    "\n",
    "  # Load the OWL ontology\n",
    "  onto = get_ontology(ontology_file_path).load()\n",
    "  ontology = onto\n",
    "\n",
    "  doc = nlp(sentence)\n",
    "\n",
    "  # Prepare empty results dictionary\n",
    "  results = {\n",
    "      \"ontology\": [],\n",
    "      \"ner\": [],\n",
    "      \"dependency_parsing\": []\n",
    "  }\n",
    "\n",
    "  # Coreference resolution (using simple heuristic)\n",
    "  coref_chains = {}\n",
    "  for token in doc:\n",
    "    if token.dep_ == \"nsubj\":  # Identify potential subjects\n",
    "      entity_text = token.text\n",
    "      if entity_text in coref_chains:\n",
    "        coref_chains[entity_text].append(token.idx)\n",
    "      else:\n",
    "        coref_chains[entity_text] = [token.idx]\n",
    "\n",
    "  # Process each token\n",
    "  for token in doc:\n",
    "    text = token.text\n",
    "    pos = token.pos_\n",
    "    dep_ = token.dep_\n",
    "    head = token.head.text\n",
    "\n",
    "    # Named Entity Recognition (NER)\n",
    "    entity_text = \"\"\n",
    "    entity_type = \"\"\n",
    "    if token.ent_type_:\n",
    "      entity_text = text\n",
    "      entity_type = token.ent_type_\n",
    "    results[\"ner\"].append({\"text\": entity_text, \"type\": entity_type})\n",
    "\n",
    "    # Ontology Table with relation and attribute logic, and superclass tagging\n",
    "    if pos in [\"NOUN\", \"PROPN\"]:\n",
    "      relation = \"\"\n",
    "      attributes = []\n",
    "      superclass = None  # Initialize superclass\n",
    "\n",
    "      # Check for verb phrase as relation (consider dependency parsing)\n",
    "      if dep_ == \"ROOT\" or dep_ == \"verb\":\n",
    "        relation = head\n",
    "\n",
    "      # Identify potential attributes using part-of-speech and dependency parsing\n",
    "      for child in token.children:\n",
    "        if child.pos_ in [\"ADJ\", \"ADP\"]:  # Adjectives or adpositional phrases\n",
    "          attributes.append(child.text)\n",
    "\n",
    "      # Coreference resolution (apply to previously identified entity)\n",
    "      if token.idx in coref_chains:\n",
    "        entity_text = coref_chains[entity_text][0]  # Use first mention as reference\n",
    "        entity_text = doc[entity_text].text\n",
    "\n",
    "      # Superclass tagging using the loaded ontology\n",
    "      superclass = tag_noun_with_superclass(ontology, entity_text, higher_threshold, lower_threshold)\n",
    "\n",
    "      results[\"ontology\"].append({\n",
    "          \"text\": entity_text,\n",
    "          \"type\": \"Entity\" if entity_type else \"Potential Entity\",\n",
    "          \"relation\": relation,\n",
    "          \"attributes\": attributes,\n",
    "          \"superclass\": superclass\n",
    "      })\n",
    "\n",
    "   # Dependency Parsing\n",
    "    results[\"dependency_parsing\"].append({\n",
    "        \"word\": text,\n",
    "        \"pos\": pos,\n",
    "        \"dependency\": dep_,\n",
    "        \"head\": head\n",
    "    })\n",
    "\n",
    "  # WordNet integration (enrich ontology with hypernyms/synonyms)\n",
    "  for entity in results[\"ontology\"]:\n",
    "    if entity[\"text\"]:\n",
    "      synsets = wn.synsets(entity[\"text\"])\n",
    "      if synsets:\n",
    "        entity[\"hypernyms\"] = [synset.hypernyms() for synset in synsets][0]  # Get first hypernym\n",
    "        entity[\"synonyms\"] = [lemma.name() for synset in synsets for lemma in synset.lemmas()]\n",
    "\n",
    "  # Disambiguation (simple example using WordNet glosses)\n",
    "  for entity in results[\"ontology\"]:\n",
    "    if entity[\"relation\"] and entity[\"text\"]:\n",
    "      synsets = wn.synsets(entity[\"text\"])\n",
    "      if synsets:\n",
    "        gloss_matches = [gloss for synset in synsets for gloss in synset.definition().split() if entity[\"relation\"] in gloss]\n",
    "        if gloss_matches:\n",
    "          entity[\"relation\"] = gloss_matches[0]  # Choose first gloss match (can be improved)\n",
    "\n",
    "  return results\n",
    "\n",
    "# Example usage (modify the ontology file path accordingly)\n",
    "ontology_file_path = \"onto88.owx\"\n",
    "sentence = \"The queen is not laying any eggs\"\n",
    "analysis_result = analyze_sentence(sentence, ontology_file_path)\n",
    "\n",
    "# Print the results (further processing might be needed)\n",
    "print(\"Ontology Table:\")\n",
    "print(analysis_result[\"ontology\"])\n",
    "print(\"\\nNER Table:\")\n",
    "print(analysis_result[\"ner\"])\n",
    "print(\"\\nDependency Parsing Table:\")\n",
    "print(analysis_result[\"dependency_parsing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>dependency</th>\n",
       "      <th>head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td>queen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>queen</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>laying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td>laying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not</td>\n",
       "      <td>PART</td>\n",
       "      <td>neg</td>\n",
       "      <td>laying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>laying</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>laying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td>eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eggs</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>laying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word   pos dependency    head\n",
       "0     The   DET        det   queen\n",
       "1   queen  NOUN      nsubj  laying\n",
       "2      is   AUX        aux  laying\n",
       "3     not  PART        neg  laying\n",
       "4  laying  VERB       ROOT  laying\n",
       "5     any   DET        det    eggs\n",
       "6    eggs  NOUN       dobj  laying"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# turn into dataframe\n",
    "df_namess= pd.DataFrame(analysis_result[\"dependency_parsing\"])\n",
    "df_namess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_dataframes_from_analysis(analysis_result):\n",
    "    \"\"\"\n",
    "    Generates separate DataFrames for verbs, nouns, and relations.\n",
    "\n",
    "    Handles potential missing keys in entities and uses compatible methods\n",
    "    for DataFrame creation and updates for older pandas versions.\n",
    "\n",
    "    Args:\n",
    "        analysis_result (dict): The dictionary containing the results from\n",
    "            the analyze_sentence function.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames (verbs, nouns, relations).\n",
    "    \"\"\"\n",
    "\n",
    "    df_verbs = pd.DataFrame(columns=[\"id\", \"word\", \"pos\", \"dependency\", \"head\"])\n",
    "    df_nouns = pd.DataFrame(columns=[\"id\", \"text\", \"type\", \"relation\", \"attributes\", \"superclass\", \"hypernyms\", \"synonyms\"])\n",
    "    df_relations = pd.DataFrame(columns=[\"verb_id\", \"noun_id\"])\n",
    "\n",
    "    verb_id = 1\n",
    "    noun_id = 1\n",
    "\n",
    "    # Process entities from the ontology results (handle missing keys)\n",
    "    noun_data = []\n",
    "    for entity in analysis_result[\"ontology\"]:\n",
    "        noun_data.append({\n",
    "            \"id\": noun_id,\n",
    "            \"text\": entity.get(\"text\", \"\"),  # Handle missing 'text' with empty string\n",
    "            \"type\": entity.get(\"type\", \"Potential Entity\"),  # Handle missing 'type'\n",
    "            \"relation\": entity.get(\"relation\", \"\"),  # Handle missing 'relation' with empty string\n",
    "            \"attributes\": entity.get(\"attributes\", []),  # Handle missing 'attributes' with empty list\n",
    "            \"superclass\": entity.get(\"superclass\", \"\"),  # Handle missing 'superclass' with empty string\n",
    "            \"hypernyms\": entity.get(\"hypernyms\", []),  # Handle missing 'hypernyms' with empty list\n",
    "            \"synonyms\": entity.get(\"synonyms\", []),  # Handle missing 'synonyms' with empty list\n",
    "        })\n",
    "        noun_id += 1\n",
    "    df_nouns = pd.concat([df_nouns, pd.DataFrame(noun_data)], ignore_index=True)\n",
    "\n",
    "    # Process verbs from the dependency parsing results\n",
    "    verb_data = []\n",
    "    for dependency_item in analysis_result[\"dependency_parsing\"]:\n",
    "        if dependency_item[\"pos\"] == \"VERB\":\n",
    "            dependency_item[\"id\"] = verb_id\n",
    "            verb_data.append(dependency_item)\n",
    "            verb_id += 1\n",
    "    df_verbs = pd.concat([df_verbs, pd.DataFrame(verb_data)], ignore_index=True)\n",
    "\n",
    "    # Create relation entries for verbs and nouns found in the same sentence\n",
    "    for dependency_item in analysis_result[\"dependency_parsing\"]:\n",
    "        if dependency_item[\"pos\"] == \"VERB\":\n",
    "            sentence_nouns = df_nouns[df_nouns[\"text\"].isin([dependency_item[\"word\"]])]  # Pass a list to isin\n",
    "            for _, noun_row in sentence_nouns.iterrows():\n",
    "                df_relations = pd.concat([df_relations, pd.DataFrame({\"verb_id\": dependency_item[\"id\"], \"noun_id\": noun_row[\"id\"]})], ignore_index=True)\n",
    "\n",
    "    return df_verbs, df_nouns, df_relations\n",
    "\n",
    "df_verbs, df_nouns, df_relations= create_dataframes_from_analysis(analysis_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb_id</th>\n",
       "      <th>noun_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [verb_id, noun_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
